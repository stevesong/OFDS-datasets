name: Generate and Upload PMTiles to S3

on:
  push:
    branches: [main]
    paths:
      - '**/*.geojson'
  workflow_dispatch: # Allows manual triggering

jobs:
  build:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Install Tippecanoe
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential libsqlite3-dev zlib1g-dev
          git clone https://github.com/felt/tippecanoe.git
          cd tippecanoe
          make -j
          sudo make install
      
      - name: Find all GeoJSON files
        id: find-geojson
        run: echo "GEOJSON_FILES=$(find . -name '*.geojson' | tr '\n' ' ')" >> $GITHUB_ENV
      
      - name: Generate Basic PMTiles
        run: |
          tippecanoe -o ofds_networks.pmtiles -zg --drop-densest-as-needed $GEOJSON_FILES
      
      - name: Generate Layered PMTiles
        run: |
          # Process each country/operator combination as a separate layer
          for country in $(find . -mindepth 1 -maxdepth 1 -type d -not -path "*/\.*" | sort); do
            countryName=$(basename $country)
            for operator in $(find $country -mindepth 1 -maxdepth 1 -type d | sort); do
              operatorName=$(basename $operator)
              
              # Create layer name
              layerName="${countryName}_${operatorName}"
              
              # Find nodes and spans
              nodesFile=$(find $operator -name "*nodes*.geojson")
              spansFile=$(find $operator -name "*spans*.geojson")
              
              # Add to tippecanoe commands
              if [ -n "$nodesFile" ]; then
                nodeArgs="$nodeArgs -L${layerName}_nodes:$nodesFile"
              fi
              
              if [ -n "$spansFile" ]; then
                spanArgs="$spanArgs -L${layerName}_spans:$spansFile"
              fi
            done
          done
          
          # Run tippecanoe with all collected layer arguments
          tippecanoe -o ofds_networks_by_layer.pmtiles -zg --drop-densest-as-needed $nodeArgs $spanArgs
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      
      - name: Upload PMTiles to S3
        run: |
          # Upload both PMTiles files to S3
          aws s3 cp ofds_networks.pmtiles s3://${{ secrets.S3_BUCKET_NAME }}/ofds_networks.pmtiles --content-type 'application/octet-stream' --cache-control 'max-age=86400'
          aws s3 cp ofds_networks_by_layer.pmtiles s3://${{ secrets.S3_BUCKET_NAME }}/ofds_networks_by_layer.pmtiles --content-type 'application/octet-stream' --cache-control 'max-age=86400'
          
          # Generate current timestamp for the metadata file
          TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          
          # Create a metadata JSON file with timestamp and file URLs
          cat > pmtiles_metadata.json << EOL
          {
            "updated_at": "${TIMESTAMP}",
            "files": {
              "basic": "https://${{ secrets.S3_BUCKET_NAME }}.s3.${{ secrets.AWS_REGION }}.amazonaws.com/ofds_networks.pmtiles",
              "layered": "https://${{ secrets.S3_BUCKET_NAME }}.s3.${{ secrets.AWS_REGION }}.amazonaws.com/ofds_networks_by_layer.pmtiles"
            }
          }
          EOL
          
          # Upload the metadata file
          aws s3 cp pmtiles_metadata.json s3://${{ secrets.S3_BUCKET_NAME }}/pmtiles_metadata.json --content-type 'application/json' --cache-control 'max-age=300'
      